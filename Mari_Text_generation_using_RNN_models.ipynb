{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "smT71TxSy7ts",
        "BzqzVLJdzmuZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marimcmurtrie/NLP/blob/main/Mari_Text_generation_using_RNN_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Character-Level Text Generation Using basic RNN\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "Implement a basic RNN model for text generation.\n",
        "\n",
        "Train the model on a text dataset (e.g., a short novel or song lyrics).\n",
        "\n",
        "Explore the generated text samples during training and analyze how the quality improves over time.\n",
        "\n",
        "\n",
        "**Tools:**\n",
        "\n",
        "Python, PyTorch, Google Colab\n",
        "\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "Use a public domain text (e.g., Shakespeare’s plays, a short novel, or song lyrics).\n",
        "\n",
        "The text data will be converted into sequences of characters for training."
      ],
      "metadata": {
        "id": "CXwvdmcQewf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Load a text dataset from URL\n",
        "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'  # Pride and Prejudice by Jane Austen\n",
        "response = requests.get(url)\n",
        "text = response.text  # Get the text content\n",
        "\n",
        "# Use a subset of the text for quicker training\n",
        "text = text[:100000]\n",
        "print(f'Total characters in text: {len(text)}')\n",
        "\n",
        "# Create character mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "print(f'Number of unique characters: {vocab_size}')\n",
        "\n",
        "# Convert text into integer sequences\n",
        "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
        "\n",
        "# Hyperparameters\n",
        "seq_length = 100  # Length of input sequences for training\n",
        "hidden_size = 128  # Number of hidden units in RNN\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create input-output pairs\n",
        "def create_sequences(data, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data) - seq_length):\n",
        "        inputs.append(data[i:i + seq_length])\n",
        "        targets.append(data[i + seq_length])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "inputs, targets = create_sequences(encoded_text, seq_length)\n",
        "print(f'Number of sequences: {len(inputs)}')\n",
        "\n",
        "# Define dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Create data loader\n",
        "dataset = TextDataset(inputs, targets)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNTextGenerator, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)  # Convert input indices to embeddings\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # Predict the next character\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "# Instantiate the model\n",
        "model = RNNTextGenerator(vocab_size, hidden_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "# Training function with dynamic hidden state initialization\n",
        "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, targets in data_loader:\n",
        "            batch_size = inputs.size(0)  # Get the actual batch size\n",
        "            hidden = model.init_hidden(batch_size).to(device)  # Initialize hidden state for each batch\n",
        "\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            hidden = hidden.detach()  # Detach hidden states to prevent gradient backpropagation through the entire history\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(\"Generated text sample:\")\n",
        "            print(generate_text(model, \"It is a truth universally acknowledged\", 200))\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(model, start_string, length=100):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[ch] for ch in start_string], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1).to(device)\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        output_dist = torch.softmax(output, dim=1).data\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = idx_to_char[top_char.item()]\n",
        "        generated_text += predicted_char\n",
        "        input_seq = torch.tensor([[top_char.item()]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Train the model\n",
        "train(model, data_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "# Testing the model by generating text with a given prompt\n",
        "start_string = \"It is a truth universally acknowledged\"  # Starting prompt\n",
        "generated_length = 500  # Length of text to generate\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generate_text(model, start_string, generated_length))\n",
        "\n"
      ],
      "metadata": {
        "id": "RLc-PdSrxzT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c80635f-5c3c-4f0e-d192-0972715e8238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters in text: 100000\n",
            "Number of unique characters: 90\n",
            "Number of sequences: 99900\n",
            "Epoch 1/20, Loss: 1.8689\n",
            "Epoch 2/20, Loss: 1.5042\n",
            "Epoch 3/20, Loss: 1.7252\n",
            "Epoch 4/20, Loss: 1.6972\n",
            "Epoch 5/20, Loss: 1.4188\n",
            "Generated text sample:\n",
            "It is a truth universally acknowledged, and room_.”\n",
            "\n",
            "“Hot eld\n",
            "on he\n",
            "beendriss exceshed his. Mary withde is if gleem” SI dested and a bage-melase wedones; Mrs. PR; youint, and fire, and prainely; but offunions Mr. Dianring\n",
            "danced schu\n",
            "Epoch 6/20, Loss: 1.2501\n",
            "Epoch 7/20, Loss: 1.7192\n",
            "Epoch 8/20, Loss: 1.0638\n",
            "Epoch 9/20, Loss: 1.5249\n",
            "Epoch 10/20, Loss: 1.1964\n",
            "Generated text sample:\n",
            "It is a truth universally acknowledged, I with\n",
            "Mr. Jare any bullso, with pecas was discried been the rust. I do not_.”\n",
            "\n",
            "[Illustraric her acculical, at any foo not chard: drejure a great is itself in the charmed by Gut that any for anse\n",
            "Epoch 11/20, Loss: 1.5564\n",
            "Epoch 12/20, Loss: 1.2456\n",
            "Epoch 13/20, Loss: 1.1224\n",
            "Epoch 14/20, Loss: 1.2314\n",
            "Epoch 15/20, Loss: 1.4638\n",
            "Generated text sample:\n",
            "It is a truth universally acknowledged hoped more difficulbying, I think of the character.”\n",
            "\n",
            "“I, me not womened, as any\n",
            "her; come; and “that is and good,\n",
            "sunianty, to think\n",
            "liked him, and Mrs. Bingley soord, and that it if cince\n",
            "for\n",
            "Epoch 16/20, Loss: 1.4978\n",
            "Epoch 17/20, Loss: 1.1873\n",
            "Epoch 18/20, Loss: 1.3329\n",
            "Epoch 19/20, Loss: 1.0186\n",
            "Epoch 20/20, Loss: 1.2309\n",
            "Generated text sample:\n",
            "It is a truth universally acknowledged refleedd to dribting\n",
            "excell to sidenise their mine\n",
            "in it is and I was recellen. Coll\n",
            "cansequeation when Jane; but Mr.\n",
            "Bennet\n",
            "with more right_ dyep; being life. I\n",
            "in you, for a Darcy, imseld as\n",
            "Generated Text:\n",
            "It is a truth universally acknowledged now of\n",
            "they as her more every\n",
            "grown tever stirisher a large if you head so liker to his fin to be immedial sensome, undly be so\n",
            "were an admiration: “You\n",
            "them”\n",
            "\n",
            "                          35\n",
            "\n",
            "When progsent\n",
            "with Elizabeth\n",
            "will at laving, thougforming her\n",
            "at\n",
            "for are equreet\n",
            "to provoo part, it should not a few convexmed have recommon; and Elizabeth, I had he such\n",
            "of answer: when but in its scilt of in their time\n",
            "is not in his sistor, and any to more done to, he is\n",
            "“belous.”\n",
            "\n",
            "“My de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**\n",
        "- In the RNNTextGenerator class, self.fc is a fully connected (linear) layer that maps the hidden state output from the RNN to the vocabulary size, enabling the model to make predictions over the possible output characters.\n",
        "\n",
        "- Purpose of self.fc:\n",
        "    - Input to self.fc: The RNN layer (self.rnn) outputs a hidden state of size [batch_size, hidden_size], which encapsulates the learned information from the input sequence.\n",
        "\n",
        "    - Role of self.fc: self.fc is a fully connected layer (nn.Linear) with dimensions [hidden_size, vocab_size]. It maps the RNN’s hidden state to a vector of size vocab_size (one element for each possible character in the vocabulary).\n",
        "\n",
        "    - Output: The output of self.fc is a logit vector of length vocab_size, which represents the unnormalized scores for each possible character. When a softmax function is applied to this vector, it converts the scores into a probability distribution, where each element represents the likelihood of a specific character being the next in the sequence.\n",
        "\n",
        "\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "- Try changing the seq_length and hidden_size. How does it affect the quality of the generated text?\n",
        "\n",
        "\n",
        "- Train the model for more epochs and observe how the generated text quality improves.\n",
        "\n",
        "\n",
        "- Replace the RNN layer with an LSTM or GRU layer and compare the difference in generated text quality."
      ],
      "metadata": {
        "id": "LJX5i-t6x5_R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iunAJj07x_D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Character-Level Text Generation with LSTM\n",
        "\n",
        " This is another version of the character-level text generation using LSTM instead of a simple RNN.\n",
        "\n",
        " LSTMs (Long Short-Term Memory networks) are well-suited for text generation because they can better capture long-term dependencies in sequential data, making them ideal for generating more coherent and context-aware text"
      ],
      "metadata": {
        "id": "Bao242xzyDOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File: lstm_text_generation.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Load a text dataset from URL\n",
        "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'  # Pride and Prejudice by Jane Austen\n",
        "response = requests.get(url)\n",
        "text = response.text[:20000]  # Get the text content; use a seubset to speedup the demo\n",
        "print(f'Total characters in text: {len(text)}')\n",
        "\n",
        "# Create character mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "print(f'Number of unique characters: {vocab_size}')\n",
        "\n",
        "# Convert text into integer sequences\n",
        "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
        "\n",
        "# Hyperparameters\n",
        "seq_length = 30  # 100 Length of input sequences for training\n",
        "hidden_size = 256  # Number of hidden units in LSTM\n",
        "batch_size = 16  #batch size: 64\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create input-output pairs\n",
        "def create_sequences(data, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data) - seq_length):\n",
        "        inputs.append(data[i:i + seq_length])\n",
        "        targets.append(data[i + seq_length])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "inputs, targets = create_sequences(encoded_text, seq_length)\n",
        "print(f'Number of sequences: {len(inputs)}')\n",
        "\n",
        "\n",
        "# Define dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Create data loader\n",
        "dataset = TextDataset(inputs, targets)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the LSTM-based model for text generation\n",
        "class LSTMTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(LSTMTextGenerator, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)  # Convert input indices to embeddings\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # Predict the next character\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
        "                torch.zeros(1, batch_size, self.hidden_size))\n",
        "\n",
        "# Instantiate the model\n",
        "model = LSTMTextGenerator(vocab_size, hidden_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, targets in data_loader:\n",
        "            batch_size = inputs.size(0)  # Get the actual batch size for each batch\n",
        "            hidden = model.init_hidden(batch_size)  # Initialize hidden state dynamically\n",
        "            hidden = tuple([h.to(device) for h in hidden])  # Move each element of hidden to the device\n",
        "\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            hidden = tuple([h.detach() for h in hidden])  # Detach hidden states to prevent gradient backpropagation through the entire history\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(\"Generated text sample:\")\n",
        "            print(generate_text(model, \"It is a truth universally acknowledged\", 200))\n",
        "\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(model, start_string, length=100):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[ch] for ch in start_string], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1)\n",
        "    hidden = tuple([h.to(device) for h in hidden])\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        output_dist = torch.softmax(output, dim=1).data\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = idx_to_char[top_char.item()]\n",
        "        generated_text += predicted_char\n",
        "        input_seq = torch.tensor([[top_char.item()]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Train the model\n",
        "train(model, data_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "# Testing the model by generating text with a given prompt\n",
        "start_string = \"It is a truth universally acknowledged\"  # Starting prompt\n",
        "generated_length = 500  # Length of text to generate\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generate_text(model, start_string, generated_length))\n"
      ],
      "metadata": {
        "id": "vLFORn7EyPAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c27233-06ae-449e-babd-0cb897eacd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters in text: 20000\n",
            "Number of unique characters: 77\n",
            "Number of sequences: 19970\n",
            "Epoch 1/5, Loss: 1.5721\n",
            "Epoch 2/5, Loss: 0.1734\n",
            "Epoch 3/5, Loss: 2.8040\n",
            "Epoch 4/5, Loss: 3.0650\n",
            "Epoch 5/5, Loss: 1.2011\n",
            "Generated text sample:\n",
            "It is a truth universally acknowledged feacte\n",
            "ertwily but it sit prelove. Whic which tour those curison about whethir the mer stry of Swift, wife for was the not sto lespined the momore day be benusent was the mith confusion a lading out\n",
            "Generated Text:\n",
            "It is a truth universally acknowledged manratiorly _and love.\n",
            "Froens be, who deling in  o doe been rateritalit in and pishcessens, a dould_ bstifule are of the ficting,\n",
            "it\n",
            "withes a fonit\n",
            "must obily a plateent But though founse\n",
            "bult ough nouritistings of the bent us it it would in he world,\n",
            "des\n",
            "Fround by presher, of he himself. But glenater\n",
            "uffiely the cold by that is uperfites\n",
            "like scenie\n",
            "thigress westen\n",
            "ally the farimess,\n",
            "to poyessed pablifes centapher have\n",
            "as usel’squne natter_ I persce--feen the of it\n",
            "that us feleat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Character-level text generation using a GRU\n",
        "\n",
        "GRUs are simpler than LSTMs but can be similarly effective in capturing sequential dependencies in text. This example provides an additional option for students to compare the performance and text quality of GRU versus LSTM and RNN.\n",
        "\n",
        "**GRU Model Architecture:**\n",
        "\n",
        "- **Embedding Layer:** Maps each character index to an embedding vector of size hidden_size.\n",
        "\n",
        "- **GRU Layer:** A single GRU layer processes the embedding sequence. Unlike LSTM, GRU does not have a separate cell state, making it computationally simpler and faster.\n",
        "\n",
        "- **Fully Connected Layer:** Maps the GRU’s hidden state to the vocabulary size, predicting the next character in the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "nJcm4epUyZFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File: gru_text_generation.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Load a text dataset (Pride and Prejudice by Jane Austen)\n",
        "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text[:20000]  # Use a smaller subset of text for quicker training\n",
        "print(f'Total characters in text: {len(text)}')\n",
        "\n",
        "# Create character mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "print(f'Number of unique characters: {vocab_size}')\n",
        "\n",
        "# Convert text into integer sequences\n",
        "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
        "\n",
        "# Hyperparameters\n",
        "seq_length = 30  # Length of input sequences for training\n",
        "hidden_size = 128  # Number of hidden units in GRU\n",
        "batch_size = 16  # Smaller batch size\n",
        "num_epochs = 5  # Fewer epochs for quicker training\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create input-output pairs\n",
        "def create_sequences(data, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data) - seq_length):\n",
        "        inputs.append(data[i:i + seq_length])\n",
        "        targets.append(data[i + seq_length])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "inputs, targets = create_sequences(encoded_text, seq_length)\n",
        "print(f'Number of sequences: {len(inputs)}')\n",
        "\n",
        "# Define dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Create data loader\n",
        "dataset = TextDataset(inputs, targets)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the GRU-based model for text generation\n",
        "class GRUTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(GRUTextGenerator, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)  # Convert input indices to embeddings\n",
        "        out, hidden = self.gru(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # Predict the next character\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "# Instantiate the model\n",
        "model = GRUTextGenerator(vocab_size, hidden_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function with dynamic hidden state initialization\n",
        "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, targets in data_loader:\n",
        "            batch_size = inputs.size(0)  # Get the actual batch size\n",
        "            hidden = model.init_hidden(batch_size).to(device)  # Initialize hidden state dynamically\n",
        "\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            hidden = hidden.detach()  # Detach hidden states to prevent backpropagation through entire history\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(\"Generated text sample:\")\n",
        "            print(generate_text(model, \"It is a truth universally acknowledged\", 200))\n",
        "\n",
        "# Function to generate text with GRU-based model\n",
        "def generate_text(model, start_string, length=100):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[ch] for ch in start_string], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1).to(device)\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        output_dist = torch.softmax(output, dim=1).data\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = idx_to_char[top_char.item()]\n",
        "        generated_text += predicted_char\n",
        "        input_seq = torch.tensor([[top_char.item()]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Train the model\n",
        "train(model, data_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "# Testing the model by generating text with a given prompt\n",
        "start_string = \"It is a truth universally acknowledged\"  # Starting prompt\n",
        "generated_length = 500  # Length of text to generate\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generate_text(model, start_string, generated_length))\n"
      ],
      "metadata": {
        "id": "z-cIC0yRykq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d007bfc-f55f-452e-df34-ce9bb675f434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters in text: 20000\n",
            "Number of unique characters: 77\n",
            "Number of sequences: 19970\n",
            "Epoch 1/5, Loss: 1.3980\n",
            "Epoch 2/5, Loss: 0.3977\n",
            "Epoch 3/5, Loss: 1.5277\n",
            "Epoch 4/5, Loss: 2.2150\n",
            "Epoch 5/5, Loss: 2.2596\n",
            "Generated text sample:\n",
            "It is a truth universally acknowledged delling and has\n",
            "doovev. Cos bout doe inserulits gurhesseld, appersons bes not englanor pirifaced be\n",
            "kinvinits I\n",
            "tooo frial at and liked what tDarmal, yerough genatten rlain Bense have\n",
            "olo dould i\n",
            "Generated Text:\n",
            "It is a truth universally acknowledged to boudh bifferess--some moraled in and uraloges, to culd ssubsouctions of ruses one tul. And und givinibot who may eatifratious nodel the utoling, his a forles;\n",
            "workes and not knowonded and to hive as nogman and and gracy alminit. It ive, ame inder be exhacter; greal and in other of _as mout are\n",
            "dould is thing brop in one of chot poy couth, _all doop;\n",
            "nears, than and glin coulde author byIt in the charrpio8d,) and propherent, in and in custen putis caled and indeed of pain exhing delives, a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Temperature Sampling for Text Generation\n",
        "\n",
        "Temperature sampling is a technique used to control the randomness of the predictions during text generation, especially in language models like RNNs, LSTMs, and GRUs. This technique can help adjust the confidence of a model when selecting the next character or word in a sequence."
      ],
      "metadata": {
        "id": "smT71TxSy7ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File: gru_text_generation_with_temperature.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Load a text dataset from URL\n",
        "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'  # Pride and Prejudice by Jane Austen\n",
        "response = requests.get(url)\n",
        "text = response.text  # Get the text content\n",
        "print(f'Total characters in text: {len(text)}')\n",
        "\n",
        "# Create character mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "print(f'Number of unique characters: {vocab_size}')\n",
        "\n",
        "# Convert text into integer sequences\n",
        "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
        "\n",
        "# Hyperparameters\n",
        "seq_length = 100  # Length of input sequences for training\n",
        "hidden_size = 256  # Number of hidden units in GRU\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create input-output pairs\n",
        "def create_sequences(data, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data) - seq_length):\n",
        "        inputs.append(data[i:i + seq_length])\n",
        "        targets.append(data[i + seq_length])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "inputs, targets = create_sequences(encoded_text, seq_length)\n",
        "print(f'Number of sequences: {len(inputs)}')\n",
        "\n",
        "# Define dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Create data loader\n",
        "dataset = TextDataset(inputs, targets)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the GRU-based model for text generation\n",
        "class GRUTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(GRUTextGenerator, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)  # Convert input indices to embeddings\n",
        "        out, hidden = self.gru(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # Predict the next character\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "# Instantiate the model\n",
        "model = GRUTextGenerator(vocab_size, hidden_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "        hidden = hidden.to(device)\n",
        "\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            hidden = hidden.detach()  # Detach hidden states to prevent gradient backpropagation through entire history\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(\"Generated text sample:\")\n",
        "            print(generate_text(model, \"It is a truth universally acknowledged\", 200, temperature=0.8))\n",
        "\n",
        "# Function to generate text with temperature sampling\n",
        "def generate_text(model, start_string, length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[ch] for ch in start_string], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1).to(device)\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "\n",
        "        # Adjust output with temperature\n",
        "        output = output / temperature\n",
        "        output_dist = torch.softmax(output, dim=1).data\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = idx_to_char[top_char.item()]\n",
        "        generated_text += predicted_char\n",
        "        input_seq = torch.tensor([[top_char.item()]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Train the model\n",
        "train(model, data_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "# Testing the model by generating text with a given prompt\n",
        "start_string = \"It is a truth universally acknowledged\"  # Starting prompt\n",
        "generated_length = 500  # Length of text to generate\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generate_text(model, start_string, generated_length))\n"
      ],
      "metadata": {
        "id": "w3yT7VVdzA_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a368efba-c8b6-4a46-d236-905be9373897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters in text: 743375\n",
            "Number of unique characters: 92\n",
            "Number of sequences: 743275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "\n",
        "- Try generating text with different temperature values (e.g., 0.5, 1.0, 1.5). How does the diversity of the output change?\n",
        "\n",
        "\n",
        "- How does a very high temperature (e.g., 2.0) affect the coherence of the text?\n",
        "\n",
        "\n",
        "- What happens when the temperature is very low (e.g., 0.1)? Is the generated text too repetitive?\n",
        "\n",
        "\n",
        "**Analysis of Temperature Sampling:**\n",
        "\n",
        "- **High Temperature (> 1):** The model explores less probable characters, which can be useful for generating more creative and diverse text but might result in less coherent sentences.\n",
        "\n",
        "\n",
        "- **Low Temperature (< 1):** The model tends to choose the most likely next characters, leading to more predictable and conservative outputs. This can be useful for generating formal or structured text.\n",
        "\n",
        "\n",
        "- **Applications:** Adjusting the temperature is often used in creative writing, dialogue generation, and poetry generation where balancing between creativity and coherence is important."
      ],
      "metadata": {
        "id": "i4U2JwxozJBG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "irBAU7_OzepQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Word-Level Text Generation using an LSTM\n",
        "\n",
        "For this coding activity, we will focus on a word-level LSTM model to generate text. The model will:\n",
        "\n",
        "- Process sequences of words rather than characters.\n",
        "- Use pre-trained word embeddings (like GloVe) to represent words as vectors.\n",
        "- Generate text by predicting the next word in a sequence."
      ],
      "metadata": {
        "id": "BzqzVLJdzmuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File: word_level_text_generation.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from collections import Counter\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Load a text dataset from URL\n",
        "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'  # Pride and Prejudice by Jane Austen\n",
        "response = requests.get(url)\n",
        "text = response.text  # Get the text content\n",
        "print(f'Total characters in text: {len(text)}')\n",
        "\n",
        "# Tokenize text into words\n",
        "words = text.split()\n",
        "print(f'Total words in text: {len(words)}')\n",
        "\n",
        "# Build a vocabulary and create word-to-index mappings\n",
        "word_counts = Counter(words)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "print(f'Vocabulary size: {vocab_size}')\n",
        "\n",
        "# Convert text into integer sequences\n",
        "encoded_text = [word_to_idx[word] for word in words]\n",
        "\n",
        "# Hyperparameters\n",
        "seq_length = 10  # Length of input sequences (in words)\n",
        "hidden_size = 256  # Number of hidden units in LSTM\n",
        "embedding_dim = 100  # Embedding dimension size\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create input-output pairs\n",
        "def create_sequences(data, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data) - seq_length):\n",
        "        inputs.append(data[i:i + seq_length])\n",
        "        targets.append(data[i + seq_length])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "inputs, targets = create_sequences(encoded_text, seq_length)\n",
        "print(f'Number of sequences: {len(inputs)}')\n",
        "\n",
        "# Define dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Create data loader\n",
        "dataset = TextDataset(inputs, targets)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the LSTM-based model for word-level text generation\n",
        "class LSTMTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        super(LSTMTextGenerator, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)  # Convert input indices to word embeddings\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # Predict the next word\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
        "                torch.zeros(1, batch_size, self.hidden_size))\n",
        "\n",
        "# Instantiate the model\n",
        "model = LSTMTextGenerator(vocab_size, embedding_dim, hidden_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training function\n",
        "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "        hidden = tuple([h.to(device) for h in hidden])\n",
        "\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            hidden = tuple([h.detach() for h in hidden])  # Detach hidden states\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(\"Generated text sample:\")\n",
        "            print(generate_text(model, [\"it\", \"is\", \"a\", \"truth\"], 20))\n",
        "\n",
        "# Function to generate text with word-level model\n",
        "def generate_text(model, start_words, length=10):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([word_to_idx[word] for word in start_words], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1)\n",
        "    hidden = tuple([h.to(device) for h in hidden])\n",
        "    generated_text = start_words\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        output_dist = torch.softmax(output, dim=1).data\n",
        "        top_word = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_word = idx_to_word[top_word.item()]\n",
        "        generated_text.append(predicted_word)\n",
        "        input_seq = torch.tensor([[top_word.item()]], dtype=torch.long).to(device)\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Train the model\n",
        "train(model, data_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "# Testing the model by generating text with a given prompt\n",
        "start_string = \"It is a truth universally acknowledged\"  # Starting prompt\n",
        "generated_length = 500  # Length of text to generate\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generate_text(model, start_string, generated_length))\n"
      ],
      "metadata": {
        "id": "GdupiN_Wz6pL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}