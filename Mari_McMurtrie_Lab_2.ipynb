{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marimcmurtrie/NLP/blob/main/Mari_McMurtrie_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab2: Mari McMurtrie**\n",
        "1.   Download imdb dataset from hugging face (the train split)\n",
        "2.   Create several functions:\n",
        "- Sentence tokenization function\n",
        "  - Apply sentence tokenization to the text\n",
        "  - Return a dataframe that expands each text into multiple rows for each sentence (e.g., if the first\n",
        "text has five sentences, there are now five rows for the original text)\n",
        "- Text cleaning function\n",
        "  - Remove non-alphanumeric characters\n",
        "  - Remove stop words\n",
        "  - Lemmatize text\n",
        "  - Returns: cleaned text\n",
        "  - Apply to each row, Create a new column with the cleaned text\n",
        "- Vectorization function\n",
        "  - Returns bigram document term matrix\n",
        "  - Returns Tf-idf score vectors/matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "nnDPcElk1IlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download imdb dataset from hugging face (the train split)**\n"
      ],
      "metadata": {
        "id": "Taivn7OQjRr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_dataset('imdb', split='train')\n",
        "imdb_df = pd.DataFrame(dataset)"
      ],
      "metadata": {
        "id": "wdwgk6kJjEXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_df.head()"
      ],
      "metadata": {
        "id": "BiY7Ff4Y4c6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_df.info()  # 250k rows"
      ],
      "metadata": {
        "id": "kulvPjnX4gHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_df['label'].unique()  # 'label' contains only 1 and 0..."
      ],
      "metadata": {
        "id": "t6xAKgkf6Dwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(dataset))\n",
        "print(dataset.column_names)\n",
        "print(type(dataset['text']))\n",
        "print(dataset[0]['text'])"
      ],
      "metadata": {
        "id": "nLU4YEDNfrnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence tokenization function**\n",
        "\n",
        "\n",
        "*   Apply sentence tokenization to the text\n",
        "\n",
        "*   Return a dataframe that expands each text into multiple rows for each sentence (e.g., if the first text has five sentences, there are now five rows for the original text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TY4nuH-3jXH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import time\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "start = time.time() # this takes time!!\n",
        "data = []  # Will have a list of tuple(a_sentence, label)\n",
        "for row in dataset:\n",
        "  sentences = sent_tokenize(row['text'])\n",
        "  for sentence in sentences:\n",
        "    #print(f\"{sentence = } \")\n",
        "    data.append(\n",
        "        {'text': sentence, 'label': row['label']}\n",
        "    )\n",
        "\n",
        "sentence_imdb_df = pd.DataFrame(data)\n",
        "\n",
        "end = time.time()\n",
        "elapsed_time = int(end - start)/60\n",
        "print(f\"It took {elapsed_time} minutes to process\")\n",
        "sentence_imdb_df.head()\n"
      ],
      "metadata": {
        "id": "eRPk0ar9joEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_imdb_df.info()"
      ],
      "metadata": {
        "id": "QKe44_jkKAsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text cleaning function**\n",
        "\n",
        "* Remove non-alphanumeric characters\n",
        "* Remove stop words\n",
        "* Lemmatize text\n",
        "* Returns: cleaned text\n",
        "* Apply to each row, Create a new column with the cleaned text\n"
      ],
      "metadata": {
        "id": "jYw9Tdf8jomK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_imdb_df.head()"
      ],
      "metadata": {
        "id": "wjxATsNoubcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "n74rkzZ5jzEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def normalize_text(corpus: list[str], lemmatizer:WordNetLemmatizer) -> list[str]:\n",
        "  normalized_corpus: list[str] = []\n",
        "  for sentence in corpus:\n",
        "    # Remove non-alphanumeric characters\n",
        "    alpha_numeric_sentence =re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)\n",
        "    # Lower case words.\n",
        "    alpha_numeric_sentence = alpha_numeric_sentence.lower()\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(alpha_numeric_sentence)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    # Lemmatize text\n",
        "    lemmatized_sentence = \" \".join([lemmatizer.lemmatize(word) for word in filtered_words])\n",
        "    normalized_corpus.append(lemmatized_sentence)\n",
        "  return normalized_corpus\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "corpus: list[str] = sentence_imdb_df['text'].tolist()\n",
        "sentence_imdb_df['normalized'] = normalize_text(corpus, lemmatizer)\n",
        "sentence_imdb_df.tail()\n",
        "\n",
        "# X = vectorizer.fit_transform(corpus)\n",
        "#   print(vectorizer.get_feature_names_out())\n",
        "\n"
      ],
      "metadata": {
        "id": "3x55ej0G3VJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization function:**\n",
        "* Returns:\n",
        "  * bigram document term matrix\n",
        "  * Tf-idf score vectors/matrix\n"
      ],
      "metadata": {
        "id": "_R7Xbk1sj_YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram document term matrix\n",
        "countVectorizer = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(2, 2))\n",
        "normalized_coprpus = sentence_imdb_df['normalized'].tolist()\n",
        "print(normalized_coprpus[0:10])\n",
        "X = countVectorizer.fit_transform(normalized_coprpus)\n",
        "print(\"Bigram Document Term Matrix in Sparse Matrix\")\n",
        "print(X.shape)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "BnEV8iIkkK3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine countVectorizer (bigram) with the normalized_coprpus\n",
        "count = 0\n",
        "for key, value in countVectorizer.vocabulary_.items():\n",
        "    if count < 10:\n",
        "        print(f\"{key}: {value}\")\n",
        "        count += 1\n",
        "    else:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "umLnWfNUyYPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf score vectors/matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfVectorizer = TfidfVectorizer()\n",
        "Xt = tfidfVectorizer.fit_transform(normalized_coprpus)\n",
        "print(Xt.shape)\n",
        "print(Xt)"
      ],
      "metadata": {
        "id": "jJzMYrfrVD3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}